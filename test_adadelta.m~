clear; close all;
rng('default');
%load data
data = load('./heart.mat');
data = data.data;
data = data(1:2:100,:);
[n,d] = size(data);
n_test = fix(n/5);%used to test our algo.
label = data(1:n,1);
test_label = data(1:n_test,1);
training_label = label(n_test+1:n,:);
%label(label==2) = -1;% all the labels are +1 or -1.
test_data = data(1:n_test,2:d);
training_data = data(n_test+1:n,2:d);
[n_train,d] = size(training_data);







%% initialize variables
T =10;
train_loss = zeros(T,1);
test_loss = zeros(T,1);
num_nodes_nn = fix(n);

w1 = -10*rand(1,num_nodes_nn);
w2 = randn(num_nodes_nn,n);
b1 = -10;
b2 = randn(num_nodes_nn,1);


%Ada-delta optimization method
rho = 0.95; eepsilon = 1e-6;
E_g_square_old = zeros(n+n*n,1);
E_delta_x_square_old = zeros(n+n*n,1);
E_g_square_old_dual_w1 = zeros(1,num_nodes_nn);
E_g_square_old_dual_w2 = zeros(num_nodes_nn,n);
E_g_square_old_dual_b1 = 0;
E_g_square_old_dual_b2 = zeros(num_nodes_nn,1);
E_delta_x_square_old_dual_w1 = zeros(1,num_nodes_nn);
E_delta_x_square_old_dual_w2 = zeros(num_nodes_nn,n);
E_delta_x_square_old_dual_b1 = 0;
E_delta_x_square_old_dual_b2 = zeros(num_nodes_nn,1);

%SGD optimization method
%alpha_0 = 1e-10;% learning rate for the primal update
%beta_0 =1e-8;%learning rate for the dual update

pair_dist = zeros(n*n,1);
for i=1:n
    for j=1:n
        if i==j
            continue;
        end
        pair_dist((i-1)*n+j,:) = log(norm(data(i,2:d+1)-data(j,2:d+1)));
    end
end
pair_dist_ordering = sort(pair_dist);
mu_0 = pair_dist_ordering(fix((n*n-n)/2));% hyper-parameter for sampling w
%sigma_0 = 3*var(pair_dist_ordering);%hyper-parameter for sampling w
sigma_0 = 1;




tLaplace_inference(training_data, n_train, training_label, mu_0, u_0, tau)


%%initialize mu and L
%theta =[zeros(n,1); reshape(eye(n),n*n,1)];% use constant to initialize
%theta = load('theta.mat');%use the end of previous train to initialize
%theta = theta.theta;
%compute the kernel matrice to initialize L
Knn_expectation = zeros(n,n);%ARD kernel matrix
for i=1:n
    for j=1:n
        pair_diff = data(i,2:d+1) - data(j,2:d+1);
        Knn_expectation(i,j) = exp(-1/2*pair_diff * diag(1 ./ (exp(mu_0)*ones(d,1))) * pair_diff')+ 1e-3;
    end
end
Knn_expectation_logdet = logdet(Knn_expectation);
Knn_expectation_inv = inv(Knn_expectation);
L_temp = chol(Knn_expectation,'lower');
theta = [[zeros(n_test,1);training_label]; reshape( L_temp, n*n,1)];%initialize theta
%theta = [zeros(n,1); reshape( L_temp, n*n,1)];%initialize theta
mu_temp = theta(1:n,:);
L_temp = theta(n+1:n+n*n,:);
L_temp = reshape(L_temp,n,n);
%to match the y and the gradient of g1
p_alpha_v_w_expectation = 1e-18*exp(-n/2*log(2*3.14159)-1/2*Knn_expectation_logdet-1/2*transpose(mu_temp+L_temp*zeros(n,1))*Knn_expectation_inv*(mu_temp+L_temp*zeros(n,1)));
%parameters are saved for using in classic gp classification
u_save = zeros(d, T);
for t=1:T
    if mod(t, 500) == 0 
        disp(t);
    end
    %sample v, w
    logw = normrnd(mu_0,sigma_0,d+2,1);
    w = exp(logw);
    %w = exp(mu_0*ones(d+2,1));
    %u_0 = 1;%%%NOTICE
    u_0 = exp(randn(1));
    u = w(2:d+1,:);
    %u = ones(d,1);
    %tau = w(d+2,:);
    tau = 1e-3;
    u_save(:,t) = u;
    %compute the kernel matrice
    kernel_temp = (data(:,2:d+1) .* repmat(transpose(1 ./ u),n,1)) * transpose(data(:,2:d+1));
    kernel_diag = diag(kernel_temp);
    Knn_temp = -2*kernel_temp + ones(n,n)*diag(kernel_diag) +diag( kernel_diag)*ones(n,n);
    Knn = u_0*exp(-1/2*Knn_temp)+tau;
    
    %define the auxilary matrix
    Knn_inv = inv(Knn);
    Knn_logdet = logdet(Knn);
    epsilon = randn(n,1);
   %% update the primal variable
    %compute the stochastic gradients w.r.p.t theta 
    % update rule for the primal variable: theta
    [~, nabla_g_y_theta_g] = gradient_theta( theta, epsilon,  Knn_inv, training_label, n_test, p_alpha_v_w_expectation, w1,w2,b1,b2, Knn_logdet,n);
    %set the primal step size via ada-delta method
    E_g_square_new = rho*E_g_square_old + (1-rho)*(nabla_g_y_theta_g .^ 2);
    Delta_x = -1*sqrt(E_delta_x_square_old+eepsilon) ./ sqrt(E_g_square_new+eepsilon) .* nabla_g_y_theta_g;
    E_delta_x_square_new = rho*E_delta_x_square_old + (1-rho)*(Delta_x .^ 2);
    
    theta_temp = theta + Delta_x;
    
    E_delta_x_square_old = E_delta_x_square_new;
    E_g_square_old = E_g_square_new;
    
    %set the primal step size via SGD method
    %alpha = alpha_0 /sqrt(t);
    %alpha = alpha_0;
    %theta = theta - alpha*nabla_g_y_theta_g;
    %remember: L is a low-triangle matrix
    
    mu_temp = theta_temp(1:n,:);
    %% evaluate the train loss
    train_loss_temp = 0;
    for i=n_test+1:n
        temp = label(i,:)*mu_temp(i,:);
        train_loss_temp = train_loss_temp -log(1+exp(-temp));
    end
    train_loss(t,:) = train_loss_temp/(n-n_test);
    
    if t>=2 && train_loss(t,:) < train_loss(t-1)
        % do nothing
    else
        theta = theta_temp;
        L_temp = theta(n+1:n+n*n,:);
        L_temp = reshape(L_temp,n,n);
        L_temp = tril(L_temp);
        theta(n+1:n+n*n,:) = reshape(L_temp,n*n,1);
        
    end
    %% update the dual variable
    %compute the stochastic gradients w.r.t y
    mu_temp = theta(1:n,:);
    L_temp = theta(n+1:n+n*n,:);
    L_temp = reshape(L_temp,n,n);
    %y = 1/p_alpha_v_w_expectation*w1 * (1 ./ exp(-1*(w2*epsilon+b2)))+b1;
    y = w1 * (1 ./ exp(-1*(w2*epsilon+b2)))+b1;
    %p_alpha_v_w = 1/(power(2*3.14159,n/2) * sqrt(Knn_det))*exp(-1/2*transpose(mu_temp + L_temp*epsilon) * Knn_inv*(mu_temp + L_temp*epsilon));
    p_alpha_v_w = exp(-n/2*log(2*3.14159)-1/2*Knn_logdet-1/2*transpose(mu_temp+L_temp*epsilon)*Knn_inv*(mu_temp+L_temp*epsilon));
    
    nabla_g_y_g1_temp =  1 ./ (1+exp(-1*(w2*epsilon+b2)));
    essential_temp = 1/p_alpha_v_w_expectation*p_alpha_v_w+1/y;
    nabla_g_y_w1 = nabla_g_y_g1_temp' * essential_temp;% 1 x num_nodes_nn
    
    nabla_g_y_w2_temp = w1' .* ((exp(-1*(w2*epsilon+b2))) ./ ((1+(exp(-1*(w2*epsilon+b2)))) .^ 2)); 
    nabla_g_y_w2 = essential_temp*(repmat(nabla_g_y_w2_temp,1,n) .* repmat(epsilon',num_nodes_nn,1));
    
    nabla_g_y_b1 = essential_temp;
    nabla_g_y_b2_temp = w1' .* exp(-1*(w2*epsilon+b2)) ./ ((1+exp(-1*(w2*epsilon+b2))) .^ 2);
    nabla_g_y_b2 = essential_temp*nabla_g_y_b2_temp;
    
    % ada-delta method for the dual variable: y
    E_g_square_new_dual_w1 = rho*E_g_square_old_dual_w1 + (1-rho)*(nabla_g_y_w1 .^ 2);
    E_g_square_new_dual_w2 = rho*E_g_square_old_dual_w2 + (1-rho)*(nabla_g_y_w2 .^ 2);
    E_g_square_new_dual_b1 = rho*E_g_square_old_dual_b1 + (1-rho)*(nabla_g_y_b1 .^ 2);
    E_g_square_new_dual_b2 = rho*E_g_square_old_dual_b2 + (1-rho)*(nabla_g_y_b2 .^ 2);
    
    Delta_x_dual_w1 = sqrt(E_delta_x_square_old_dual_w1+eepsilon) ./ sqrt(E_g_square_new_dual_w1+eepsilon) .* nabla_g_y_w1;
    Delta_x_dual_w2 = sqrt(E_delta_x_square_old_dual_w2+eepsilon) ./ sqrt(E_g_square_new_dual_w2+eepsilon) .* nabla_g_y_w2;
    Delta_x_dual_b1 = sqrt(E_delta_x_square_old_dual_b1+eepsilon) ./ sqrt(E_g_square_new_dual_b1+eepsilon) .* nabla_g_y_b1;
    Delta_x_dual_b2 = sqrt(E_delta_x_square_old_dual_b2+eepsilon) ./ sqrt(E_g_square_new_dual_b2+eepsilon) .* nabla_g_y_b2;
    
    E_delta_x_square_new_dual_w1 = rho*E_delta_x_square_old_dual_w1 + (1-rho)*(Delta_x_dual_w1 .^ 2);
    E_delta_x_square_new_dual_w2 = rho*E_delta_x_square_old_dual_w2 + (1-rho)*(Delta_x_dual_w2 .^ 2);
    E_delta_x_square_new_dual_b1 = rho*E_delta_x_square_old_dual_b1 + (1-rho)*(Delta_x_dual_b1 .^ 2);
    E_delta_x_square_new_dual_b2 = rho*E_delta_x_square_old_dual_b2 + (1-rho)*(Delta_x_dual_b2 .^ 2);
    
    
    w1_temp = w1 + Delta_x_dual_w1;
    w2_temp = w2 + Delta_x_dual_w2;
    b1_temp = b1 + Delta_x_dual_b1;
    b2_temp = b2 + Delta_x_dual_b2;
    y_temp = w1_temp * (1 ./ exp(-1*(w2_temp*epsilon+b2_temp)))+b1_temp;
    if y_temp>= 0 
        %disp('y');
        %disp(y_temp);
    else
        w1 = w1 + Delta_x_dual_w1;
        w2 = w2 + Delta_x_dual_w2;
        b1 = b1 + Delta_x_dual_b1;
        b2 = b2 + Delta_x_dual_b2;
        y = w1 * (1 ./ exp(-1*(w2*epsilon+b2)))+b1;
        E_delta_x_square_old_dual_w1 = E_delta_x_square_new_dual_w1;
        E_delta_x_square_old_dual_w2 = E_delta_x_square_new_dual_w2;
        E_delta_x_square_old_dual_b1 = E_delta_x_square_new_dual_b1;
        E_delta_x_square_old_dual_b2 = E_delta_x_square_new_dual_b2;
        
        E_g_square_old_dual_w1 = E_g_square_new_dual_w1;
        E_g_square_old_dual_w2 = E_g_square_new_dual_w2;
        E_g_square_old_dual_b1 = E_g_square_new_dual_b1;
        E_g_square_old_dual_b2 = E_g_square_new_dual_b2;
    end
    
    
    
    % SGD for the dual variable: y
    %beta = beta_0/sqrt(t);
    %w1 = w1+beta*nabla_g_y_w1/p_alpha_v_w_expectation;
    %w2 = w2+beta*nabla_g_y_w2/p_alpha_v_w_expectation;
    %b1 = b1+beta*nabla_g_y_b1/p_alpha_v_w_expectation;
    %b2 = b2+beta*nabla_g_y_b2/p_alpha_v_w_expectation;
    
    
    
    %% evaluate loss: for train data and test data
    %theta_avg = 1/t*sum(theta_sequence,2);
    %mu_temp = theta_avg(1:n,:);
    
    %% evaluate the test loss
    test_loss_temp = 0;
    for i=1:n_test
        temp = label(i,:)*mu_temp(i,:);
        test_loss_temp = test_loss_temp - log(1+exp(-temp));
    end
    test_loss(t,:) = test_loss_temp/n_test;

    %disp('be careful!');
    
end
u_save = mean(u_save,2);
 save('w1.mat','w1')
 save('w2.mat','w2')
 save('b2.mat','b2')
 save('b1.mat','b1')
 save('theta.mat','theta')

save('u_save.mat','u_save');
save('train_loss.mat','train_loss');
save('test_loss.mat','test_loss');


    
    
